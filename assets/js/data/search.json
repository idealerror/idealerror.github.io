[ { "title": "Resources for passing AWS Certified Solutions Architect – Associate", "url": "/posts/AWS-Solutions-Architect-Associate-Certification/", "categories": "Amazon, Certification", "tags": "amazon, personaldev, certification", "date": "2021-12-29 15:00:00 -0800", "snippet": "I wanted to put together a great archive of resources that I used to pass the SA Associate certification for AWS. With these resources, I was able to study for a couple hours per day in just a few weeks and pass the exam.After finishing some of the trainings, I went straight into the practice exams/study guides. Below are some of the resources I used.Training/LabsA Cloud Guru - Not FreeI had a great experience going through the A Cloud Guru training as a refresher for some things I did not know already. The classes are easy to follow and the labs are quick. The interface is very friendly to useUdemy - Not FreeIf you’re already used to Udemy, there are a TON of great classes on Udemy that can help you pass the SAA exam. I had originally watched the A Cloud Guru class on Udemy years ago prior to using their website.AWS Skill Builder - Free/PaidAWS Skill Builder has a ton of great training resources you can go through to cover a majority of the topics you need to know.Amazon AWS Training - FreeThese are the new courses offered on Amazon.com.Practice ExamsTutorialsDojo - Some Free TestsGreat practice exams!A Cloud Guru - Not FreeI found the practice test very helpful, but I think I got more value out of the TutorialsDojo practice exams.Whizlabs - Some Free TestsGreat practice exams!Study Guideskeenanromain SAA Study Guide on GitHub - FreeI found this guide extremely helpful. I reviewed it multiple times to make sure I was able to dot my I’s and cross my T’s. I highly recommend this Study Guide.SA Associate Cheat Sheet - FreeThese are the slides from this YouTube Video. This was very helpful as a quick refresherIn SummaryFind what works for you. These are the resources I used and overall I only found myself not able to answer 1 or 2 questions on the exam. The rest of the questions I had covered based on the training/guides I listed above (and working knowledge)." }, { "title": "Sunsetting AMZ.report / Big Data Learnings", "url": "/posts/Sunsetting-AMZ-Report/", "categories": "Projects", "tags": "bigdata, amazon, programming", "date": "2021-12-28 10:33:00 -0800", "snippet": "Today I sunset the AMZ.report platform. Building this platform was a lot of fun and came with a lot of learnings. I launched the platform in late 2020 but had started building the database before that. I also built a simpler version of this when I was working in a prior role at an Amazon-focused advertising agency.Some quick stats of the data I was able to aggregate: Stat Value Search Term Reports (day/month/quarter) 840+ Unique Search Terms 7.5M+ Unique ASINs 9.8M+ Unique Ranking Rows 838M+ While it was fun to build the tool and maintain it, there was some costs associated. Primarily with storing the data in a database. Below are some average costs of the services utilized. Service Cost Database/Storage (RDS) ~$300/mo Compute (EC2) ~$40/mo AWS Average Cost ~$340/mo This was a very simplified build with no high availability, fault tolerance, etc. Storing over 1B rows in a database and running large queries on it is not cheap, especially if you expect the responses in less than a couple of seconds.This definitely benefited a lot of people who used the platform, but it did not generate any revenue, so with that, it’s been sunset.Feel free to reach out if you have any questions. My contact information can be found on the bottom of the page sidebar." }, { "title": "Running Laravel in Docker with supervisord", "url": "/posts/Running-Laravel-in-Docker-with-supervisord/", "categories": "DevOps, Docker", "tags": "laravel, docker, programming", "date": "2021-07-18 12:00:00 -0700", "snippet": "Ask any DevOps or Systems Engineer and they’ll agree that a Docker container should only run a single process. This is because of the single process dies, the container should die with it. What if you need to run multiple processes on a single container? Well that’s where supervisord comes in.What is supervisord?Supervisord is a linux application/service that allows you to run multiple applications as background processes (or foreground processes) utilizing a single service. With this, supervisord will also monitor and restart processes if they fail. Supervisord also has the ability to control When an application starts Where an application logs to What user the application runs asWhy use supervisord with Laravel?Laravel has multiple processes that utilize the same codebase such as the scheduler and worker. Supervisord makes it easy to run the web server, scheduler, and queue worker all on the same machine. If for some reason your application processes heavy loads, it may make sense to split these into multiple containers. For most simple applications though (or dev), you can just combine them into a single container.How to implement supervisord on DockerDockerfileIt’s fairly simple, you just need to make sure that you install supervisord and start it at the end of the Dockerfile.Here’s an example utilizing the image php:7.4-apacheFROM php:7.4-apache# Install supervisordRUN apt-get update &amp;amp;&amp;amp; apt-get upgrade -y &amp;amp;&amp;amp; apt-get install -y supervisor# Make supervisor log directoryRUN mkdir -p /var/log/supervisor# Copy local supervisord.conf to the conf.d directoryCOPY --chown=root:root supervisord.conf /etc/supervisor/conf.d/supervisord.conf# Start supervisordCMD [&quot;/usr/bin/supervisord&quot;, &quot;-c&quot;, &quot;/etc/supervisor/supervisord.conf&quot;]supervisord.confAs mentioned above, the supervisord.conf file contains all of the applications that will be running on the container. Below you will see that I’ve separated it out into 3 processes: worker, apache2, and schedule. Apache starts first with the priority 1, then worker, then schedule. Schedule is actually using a bash script instead of cron because cron uses it’s own environment variables.[supervisord]nodaemon = truelogfile = /dev/nulllogfile_maxbytes = 0pidfile = /run/supervisord.pid[program:worker]directory=/var/www/htmlprocess_name=%(program_name)s_%(process_num)02dcommand=php artisan queue:work --sleep=3 --tries=3autostart=trueautorestart=truestopasgroup=truekillasgroup=trueuser=rootnumprocs=2redirect_stderr=truestdout_logfile = /dev/fd/1stdout_logfile_maxbytes=0stderr_logfile = /dev/fd/2stderr_logfile_maxbytes=0redirect_stderr=truestopwaitsecs=3600priority = 6[program:apache2]command=/usr/sbin/apache2ctl -DFOREGROUNDkillasgroup=truestopasgroup=truestdout_logfile = /dev/fd/1stdout_logfile_maxbytes=0stderr_logfile = /dev/fd/2stderr_logfile_maxbytes=0redirect_stderr=trueautostart=trueautorestart=trueuser=rootpriority = 1[program:schedule]command = /bin/bash -c &quot;/var/www/html/schedule.sh&quot;stdout_logfile = /dev/fd/1stdout_logfile_maxbytes=0stderr_logfile = /dev/fd/2stderr_logfile_maxbytes=0user = rootautostart = trueautorestart = truepriority = 20schedule.shThe only major problem with using this method to run cron jobs is it doesn’t happen at the top of the minute, it happens every 60 seconds from when the script is executed. I’m sure this can be updated to start at the top of the minute, it just doesn’t matter to me much.while [ true ]do php /var/www/html/artisan schedule:run --verbose --no-interaction &amp;amp; sleep 60done" } ]
